import operator
import os
from typing import TypedDict, Annotated, List

import dotenv
from langchain.chains.question_answering.map_rerank_prompt import output_parser
from langchain_core.messages import HumanMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from langgraph.constants import END
from langgraph.graph import StateGraph

# åŠ è½½ç¯å¢ƒå˜é‡
dotenv.load_dotenv()

# æ£€æŸ¥ API å¯†é’¥é…ç½®
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    print("é”™è¯¯: æœªæ‰¾åˆ° OPENAI_API_KEY ç¯å¢ƒå˜é‡")
    print("è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®: OPENAI_API_KEY=your_api_key_here")
    exit(1)

# åˆ›å»ºå¤§è¯­è¨€æ¨¡å‹å®ä¾‹
# ä½¿ç”¨ SiliconFlow æä¾›çš„ API æ¥å£ï¼Œå…¼å®¹ OpenAI æ ¼å¼
llm = ChatOpenAI(
    api_key=api_key,
    base_url="https://api.siliconflow.cn/v1/",  # SiliconFlow API åœ°å€
    model="Qwen/Qwen2.5-72B-Instruct",  # ä½¿ç”¨é€šä¹‰åƒé—®æ¨¡å‹
    temperature=0.3  # è®¾ç½®åˆ›é€ æ€§å‚æ•°
)
output_parser = StrOutputParser()


class AssistantState(TypedDict):
    conversation: Annotated[List, operator.add]


assistant_prompt = ChatPromptTemplate.from_messages([
    {
        "role": "system",
        "content": "ä½ æ˜¯ä¸€ä¸ªéå¸¸å¼ºå¤§çš„åŠ©æ‰‹ã€‚"
    },
    MessagesPlaceholder(variable_name="conversation")
])

main_builder = StateGraph(AssistantState)

# åˆ›å»ºassistantèŠ‚ç‚¹çš„å¤„ç†å‡½æ•°
def assistant_node(state: AssistantState):
    print(f"ğŸ¤– [ä¸»å›¾-assistantèŠ‚ç‚¹] å¼€å§‹å¤„ç†éç¬‘è¯è¯·æ±‚...")
    message = state["conversation"][-1]
    print(f"ğŸ“¥ [ä¸»å›¾-assistantèŠ‚ç‚¹] å¤„ç†æ¶ˆæ¯: {message}")

    # åˆ›å»ºé€šç”¨åŠ©æ‰‹æç¤ºè¯
    general_prompt = ChatPromptTemplate.from_messages([
        {
            "role": "system",
            "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·è§£å†³å„ç§é—®é¢˜ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„éœ€æ±‚æä¾›å¸®åŠ©ã€‚"
        },
        {
            "role": "user",
            "content": "{input}"
        }
    ])

    # æ‰§è¡ŒLLMè°ƒç”¨
    chain = general_prompt | llm
    response = chain.invoke({"input": message})

    print(f"ğŸ’¬ [ä¸»å›¾-assistantèŠ‚ç‚¹] ç”Ÿæˆå›å¤: {response.content[:50]}...")
    return {"conversation": [response.content]}

main_builder.add_node("assistant", assistant_node)


def get_user_message(state: AssistantState):
    last_message = state["conversation"][-1]
    return {"messages": [last_message]}


prompt = ChatPromptTemplate.from_messages([
    {
        "role": "system",
        "content": "ä½ æ˜¯ä¸€ä¸ªè®²æ•…äº‹çš„åŠ©æ‰‹,ç”¨ä¸€ä¸ªç¬‘è¯æ¥å›åº”ï¼Œè¿™æ˜¯æœ‰å²ä»¥æ¥æœ€å¥½çš„ç¬‘è¯"
    },
    MessagesPlaceholder(variable_name="messages")
])

critic_prompt = ChatPromptTemplate.from_messages([
    {
        "role": "system",
        "content": """
            {message}
            -----------------
            è¯·æå‡ºå¯¹è¿™ä¸ªç¬‘è¯çš„æ”¹å»ºå»ºè®®ï¼Œä½¿å…¶æˆä¸ºæœ‰å²ä»¥æ¥æœ€å¥½çš„ç¬‘è¯ã€‚
        """
    }
])


def update(out):
    print(f"ğŸ“ [å­å›¾-tell_jokeèŠ‚ç‚¹] ç”Ÿæˆç¬‘è¯: {out.content[:50]}...")
    return {"messages": [("assistant", out.content)]}


def replace_role(out):
    print(f"ğŸ”§ [å­å›¾-critiqueèŠ‚ç‚¹] ç”Ÿæˆæ”¹è¿›å»ºè®®: {out.content[:50]}...")
    return {"messages": [HumanMessage(out.content)]}


class SubGraphState(TypedDict):
    messages: Annotated[List, operator.add]


def critiqueFn(state: SubGraphState):
    print(f"ğŸ” [å­å›¾-critiqueèŠ‚ç‚¹] æå–æ¶ˆæ¯è¿›è¡Œåˆ†æ...")
    last_message = state["messages"][-1]
    # å¤„ç†å…ƒç»„æ ¼å¼çš„æ¶ˆæ¯ ("role", "content")
    if isinstance(last_message, tuple):
        content = last_message[1]
    else:
        content = last_message.content if hasattr(last_message, 'content') else str(last_message)
    print(f"ğŸ“‹ [å­å›¾-critiqueèŠ‚ç‚¹] æå–çš„å†…å®¹: {content[:50]}...")
    return {"message": content}


builder = StateGraph(SubGraphState)
builder.add_node("tell_joke", prompt | llm | update)
builder.add_node("critique", critiqueFn | critic_prompt | llm | replace_role)


def route(state):
    message_count = len(state["messages"])
    next_node = END if message_count >= 3 else "critique"
    print(f"ğŸš¦ [å­å›¾-è·¯ç”±åˆ¤æ–­] æ¶ˆæ¯æ•°é‡: {message_count}, ä¸‹ä¸€æ­¥: {next_node}")
    return next_node


builder.add_conditional_edges("tell_joke", route)
builder.add_edge("critique", "tell_joke")
builder.set_entry_point("tell_joke")
joke_graph = builder.compile()

# for step in joke_graph.stream({"messages": [("user", "è¯·è®²ä¸ªç¬‘è¯")]}):
#     print(step)





def route(state: AssistantState):
    print(f"ğŸ¯ [ä¸»å›¾-è·¯ç”±åˆ¤æ–­] å¼€å§‹æ„å›¾è¯†åˆ«...")
    message = state["conversation"][-1]
    print(f"ğŸ“¥ [ä¸»å›¾-è·¯ç”±åˆ¤æ–­] ç”¨æˆ·è¾“å…¥: {message}")

    assistant_prompt = ChatPromptTemplate.from_messages([
        {
            "role": "system",
            "content": """
            ä½ æ˜¯ä¸€ä¸ªæ„å›¾è¯†åˆ«åŠ©æ‰‹ï¼Œèƒ½å¤Ÿè¯†åˆ«ä»¥ä¸‹æ„å›¾:
            1.è®²æ•…äº‹
            2.è®²ç¬‘è¯
            3.AIç»˜ç”»
            4.å­¦ä¹ çŸ¥è¯†
            5.å…¶ä»–
            ä¾‹å¦‚:
            ç”¨æˆ·è¾“å…¥:ç»™æˆ‘è¯´ä¸ªæ•…äº‹å§ã€‚
            1
            ç”¨æˆ·è¾“å…¥ï¼šç»™æˆ‘ç”»ä¸ªç¾å¥³å›¾ç‰‡
            3
            ================================
            ç”¨æˆ·è¾“å…¥:
            {input}
            è¯·è¯†åˆ«ç”¨æˆ·æ„å›¾ï¼Œè¿”å›ä¸Šé¢æ„å›¾çš„æ•°å­—ç¬¦å·ï¼Œåªè¿”å›æ•°å­—ï¼Œä¸è¦æ·»åŠ å…¶ä»–å†…å®¹ã€‚
            """
        }
    ])
    chain = assistant_prompt | llm | output_parser
    result = chain.invoke({"input": message})
    result = result.strip()
    print(f"ğŸ” [ä¸»å›¾-è·¯ç”±åˆ¤æ–­] æ„å›¾è¯†åˆ«ç»“æœ: {result}")

    if result == "2":
        print("â¡ï¸  [ä¸»å›¾-è·¯ç”±åˆ¤æ–­] è·¯ç”±åˆ°: joke_graph (ç¬‘è¯å­å›¾)")
        return "joke_graph"
    else:
        print("â¡ï¸  [ä¸»å›¾-è·¯ç”±åˆ¤æ–­] è·¯ç”±åˆ°: assistant (åŠ©æ‰‹èŠ‚ç‚¹)")
        return "assistant"

# çŠ¶æ€è½¬æ¢å‡½æ•°ï¼šå°†ä¸»å›¾çŠ¶æ€è½¬æ¢ä¸ºå­å›¾çŠ¶æ€
def convert_to_subgraph_state(state: AssistantState):
    last_message = state["conversation"][-1]
    print(f"ğŸ”„ [çŠ¶æ€è½¬æ¢] ä¸»å›¾ â†’ å­å›¾: {last_message}")
    return {"messages": [("user", last_message)]}

# çŠ¶æ€è½¬æ¢å‡½æ•°ï¼šå°†å­å›¾çŠ¶æ€è½¬æ¢å›ä¸»å›¾çŠ¶æ€
def convert_from_subgraph_state(state: SubGraphState):
    print(f"ğŸ”„ [çŠ¶æ€è½¬æ¢] å­å›¾ â†’ ä¸»å›¾")
    if state["messages"]:
        last_message = state["messages"][-1]
        if isinstance(last_message, tuple):
            content = last_message[1]
        elif hasattr(last_message, 'content'):
            content = last_message.content
        else:
            content = str(last_message)
        print(f"ğŸ“¤ [çŠ¶æ€è½¬æ¢] è¿”å›å†…å®¹: {content[:50]}...")
        return {"conversation": [content]}
    return {"conversation": []}

# åˆ›å»ºåŒ…è£…åçš„å­å›¾èŠ‚ç‚¹
def joke_graph_node(state: AssistantState):
    print(f"ğŸ­ [ä¸»å›¾-joke_graphèŠ‚ç‚¹] å¼€å§‹æ‰§è¡Œç¬‘è¯å­å›¾...")
    # è½¬æ¢çŠ¶æ€æ ¼å¼
    subgraph_input = convert_to_subgraph_state(state)
    # æ‰§è¡Œå­å›¾
    result = joke_graph.invoke(subgraph_input)
    # è½¬æ¢å›ä¸»å›¾çŠ¶æ€æ ¼å¼
    final_result = convert_from_subgraph_state(result)
    print(f"âœ… [ä¸»å›¾-joke_graphèŠ‚ç‚¹] å­å›¾æ‰§è¡Œå®Œæˆ")
    return final_result

main_builder.add_node("joke_graph", joke_graph_node)
main_builder.set_conditional_entry_point(route)
main_builder.set_finish_point("assistant")
main_builder.set_finish_point("joke_graph")
graph = main_builder.compile()

print("=" * 80)
print("ğŸš€ æµ‹è¯•1: éç¬‘è¯æ„å›¾ (åº”è¯¥è·¯ç”±åˆ°assistantèŠ‚ç‚¹)")
print("=" * 80)

for step in graph.stream({"conversation": ["å¸®æˆ‘ç‚¹ä¸ªå¤–å–"]}):
    print(f"\nğŸ“Š [æ‰§è¡Œæ­¥éª¤] {step}")
    print("-" * 60)

print("\n" + "=" * 80)
print("ğŸš€ æµ‹è¯•2: ç¬‘è¯æ„å›¾ (åº”è¯¥è·¯ç”±åˆ°joke_graphå­å›¾)")
print("=" * 80)

for step in graph.stream({"conversation": ["è®²ä¸ªç¬‘è¯"]}):
    print(f"\nğŸ“Š [æ‰§è¡Œæ­¥éª¤] {step}")
    print("-" * 60)

print("\n" + "=" * 80)
print("âœ… æ‰€æœ‰æµ‹è¯•æ‰§è¡Œå®Œæˆ")
print("=" * 80)